{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6fe8c86-2cda-4948-b28a-0bb905a1baed",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c6cb2-611e-4cac-9021-7140b63a7eb9",
   "metadata": {},
   "source": [
    "- Import required libraries\n",
    "- Load data\n",
    "- Exploratory data analysis\n",
    "- Scale data\n",
    "- Sampling\n",
    "- Train base models\n",
    "- Select Features with SelectKBest\n",
    "- Hyperparameter Tuning with Optuna\n",
    "- Model Calibration\n",
    "- Model Explanation with LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53d394-4dd5-49b7-a6d9-cc3f1c2aacbe",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9053570-aa3c-4283-bbd8-f3ed5130f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pefile scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702964fc-adc4-4b0b-9c7e-2897a5ace7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "import io\n",
    "import time\n",
    "import itertools\n",
    "import pefile\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import lime\n",
    "import shap\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import RobustScaler, OrdinalEncoder, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "if not os.path.exists(f'./outputs'):\n",
    "    os.makedirs('./outputs')\n",
    "\n",
    "PROJECT_ROOT_DIR = './outputs'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "if not os.path.exists(f'./outputs/images'):\n",
    "    os.makedirs('./outputs/images')\n",
    "    \n",
    "def save_fig(title):\n",
    "    path = os.path.join(IMAGES_PATH, title + '.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def bold(string):\n",
    "    display(Markdown(\"**\" + string + \"**\"))\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e26cb-cc74-47f7-9866-2ecfca072c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1a4c3-f444-4088-af4a-6dc1698300e8",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc880c16-7a33-4584-8a05-789daf4eb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/dataset_malwares.csv\")\n",
    "test_data = pd.read_csv(\"./data/dataset_test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121e429-3763-4b6d-8da7-f4395dcda915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(data):\n",
    "    bold(\" SHAPE \".center(50, \"#\"))\n",
    "    print(\"ROWS: {}\".format(data.shape[0]))\n",
    "    print(\"COLUMNS: {}\".format(data.shape[1]))\n",
    "    bold(\" TYPES \".center(50, \"#\"))\n",
    "    print(data.dtypes)\n",
    "    bold(\" MISSING VALUES \".center(50, \"#\"))\n",
    "    print(data.isnull().sum())\n",
    "    bold(\" DUPLICATED VALUES \".center(50, \"#\"))\n",
    "    print(\"NUMBER OF DUPLICATED VALUES: {}\".format(data.duplicated().sum()))\n",
    "    bold(\" MEMORY USAGE \".center(50, \"#\"))\n",
    "    buf = io.StringIO()\n",
    "    data.info(buf=buf)\n",
    "    info = buf.getvalue().split(\"\\n\")[-2].split(\":\")[1].strip()\n",
    "    print(\"Memory Usage: {}\".format(info))\n",
    "    bold(\" DESCRIBE \".center(50, \"#\"))\n",
    "    display(data.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0993e4-97cc-4ba3-acdc-3e18c5578227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49c29f-12d8-4568-88bf-63ba58d19ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5239806-79a3-4952-9e19-27f1169307b5",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75052-a3f1-416e-a311-358a5f1f5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_cols(df, target):\n",
    "    cat_cols = [col for col in df.columns if str(df[col].dtypes) in [\"bool\", \"category\", \"object\"] and col != target]\n",
    "    bold(f\"Categorical Variables ({len(cat_cols)})\")\n",
    "    print(cat_cols)\n",
    "    \n",
    "    num_cols = [col for col in df.columns if df[col].dtypes in [int, float] and col != target]\n",
    "    bold(f\"Numerical Variables ({len(num_cols)})\")\n",
    "    print(num_cols)\n",
    "    \n",
    "    numerical_but_categorical_cols = [col for col in num_cols if df[col].nunique() < 10 and col != target]\n",
    "    bold(f\"Numerical but Categorical Variables ({len(numerical_but_categorical_cols)})\")\n",
    "    print(numerical_but_categorical_cols)\n",
    "    \n",
    "    categorical_but_cardinal_cols = [col for col in cat_cols if df[col].nunique() > 20 and col != target]\n",
    "    bold(f\"Categorical but Cardinal Variables ({len(categorical_but_cardinal_cols)})\")\n",
    "    print(categorical_but_cardinal_cols)\n",
    "\n",
    "    same_value_cols = [col for col in df.columns if df[col].nunique() == 1 and col != target]\n",
    "    bold(f\"Same Value Variables ({len(same_value_cols)})\")\n",
    "    print(same_value_cols)\n",
    "\n",
    "    for col in same_value_cols:\n",
    "        if col in cat_cols:\n",
    "            cat_cols.remove(col)\n",
    "        elif col in num_cols:\n",
    "            num_cols.remove(col)\n",
    "        \n",
    "    return cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f836b14-b9c9-4387-ac81-76663da74581",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables, numerical_variables = grab_cols(train_data, \"Malware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325f98d-8a97-497a-bb7f-c1e463d50093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num(df, columns):\n",
    "    plt.figure(figsize=(len(columns) / 4, len(columns)))\n",
    "    for i, column in enumerate(columns):\n",
    "        plt.subplot(int(len(columns) / 2) + 1, 2, i + 1)\n",
    "        sns.histplot(x=column, data=df, bins=30, kde=True)\n",
    "        plt.axvline(df[column].mean(), color=\"r\", linestyle=\"--\", label=\"Mean\")\n",
    "        plt.axvline(df[column].median(), color=\"g\", linestyle=\"-\", label=\"Median\")\n",
    "        plt.grid()\n",
    "        plt.title(f\"{column} Distribution\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "    save_fig(\"plot_numerical_variables\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2bbcb-1fb9-464b-96bc-0c66fe417a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num(train_data, numerical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f66e0-a4a8-456d-99d2-aa1999843862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df: pd.core.frame.DataFrame, title_name: str='Correlation Map') -> None:\n",
    "    corr = df.corr(numeric_only=True)\n",
    "    fig, axes = plt.subplots(figsize=(df.shape[1], df.shape[1]))\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(corr, mask=mask, linewidths=.5, cmap='viridis', annot=True, fmt='.2f')\n",
    "    plt.title(title_name)\n",
    "    save_fig('correlation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df4c81-0e11-4642-993b-87d35030e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(train_data, \"Dataset Correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87b537-bd35-4045-9e82-9e8c2f162fc2",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578676f7-a826-4a12-9c11-79f720807fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    return [LGBMClassifier(verbose=-100),\n",
    "            XGBClassifier(),\n",
    "            RandomForestClassifier(), \n",
    "            AdaBoostClassifier(), \n",
    "            GradientBoostingClassifier(), \n",
    "            DecisionTreeClassifier(), \n",
    "            ExtraTreesClassifier(),\n",
    "            LogisticRegression(), \n",
    "            SGDClassifier(),\n",
    "            CatBoostClassifier(verbose=0), \n",
    "            MLPClassifier(),\n",
    "            GaussianNB(), \n",
    "            SVC(), \n",
    "            KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738d218-b5ff-42a9-bf17-3c39a3088659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_train(df, drop_cols, target):\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df.drop(drop_cols, axis=1)\n",
    "    y = temp_df[target]\n",
    "    X = temp_df.drop([target], axis=1)\n",
    "    \n",
    "    if not os.path.exists(f'./outputs/process'):\n",
    "        os.makedirs('./outputs/process')\n",
    "    \n",
    "    scores_df = pd.DataFrame(columns=[\"Model Name\", \"Selected Features\", \"Parameters\", \"Train Time\", \"Test Time\", \"Train Accuracy\", \"Test Accuracy\", \"F1\", \"Precision\", \"Recall\"])\n",
    "    samplers = [\"No Sampler\",\n",
    "                SMOTE(random_state=42),\n",
    "                RandomOverSampler(random_state=42),\n",
    "                ADASYN(random_state=42),\n",
    "                BorderlineSMOTE(k_neighbors=4, random_state=42)\n",
    "               ]\n",
    "\n",
    "    class_index = {}\n",
    "    for c in np.unique(y):\n",
    "        class_index[c] = np.where(y == c)[0]\n",
    "\n",
    "    min_class_len = min(len(ind) for ind in class_index.values())\n",
    "    test_index = []\n",
    "    for i in class_index.values():\n",
    "        test_index.extend(np.random.choice(i, size=min_class_len // 2, replace=False))\n",
    "\n",
    "    train_index = np.setdiff1d(X.index.values, test_index)\n",
    "    train_index = list(train_index)\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    rs = RobustScaler()\n",
    "    X_train[numerical_variables] = rs.fit_transform(X_train[numerical_variables])\n",
    "    X_test[numerical_variables] = rs.transform(X_test[numerical_variables])\n",
    "    with open('./outputs/process/rs.pkl', 'wb') as f:\n",
    "        pickle.dump(rs, f)\n",
    "    \n",
    "    class_names = temp_df[target].unique().tolist()\n",
    "\n",
    "    for sampler in samplers:\n",
    "        if sampler == \"No Sampler\":\n",
    "            sampler_name = \"No Sampler\"\n",
    "            X_train_resampled, y_train_resampled = X_train, y_train \n",
    "\n",
    "        else:\n",
    "            sampler_name = str(sampler.__class__).split(\".\")[-1].replace(\"'>\", \"\")\n",
    "            X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        models = load_models()\n",
    "        for i, model in enumerate(models):\n",
    "            train_model(i, temp_df.drop(target, axis=1), model, scores_df, X_train_resampled, X_test, y_train_resampled, y_test, class_names, sampler_name)\n",
    "\n",
    "    scores_df.to_csv(f'./outputs/scores.csv')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035fa83-3f20-410c-81f9-c31e16f8cc0c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319bc95-557f-4aee-8c7e-52ed92a8552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predicted, actuals, sub_classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    confusion = confusion_matrix(predicted, actuals)\n",
    "\n",
    "    plt.imshow(confusion, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(sub_classes))\n",
    "    plt.xticks(tick_marks, sub_classes, rotation=90)\n",
    "    plt.yticks(tick_marks, sub_classes)\n",
    "\n",
    "    for i, j in itertools.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
    "        plt.text(j, i, confusion[i, j], horizontalalignment=\"center\", color=\"black\")\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "    plt.ylabel('Actuals')\n",
    "    plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8b05-83df-4c85-ae99-760803b6748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(i, temp_df, model, scores_df, X_train, X_test, y_train, y_test, class_names, sampler_name):\n",
    "    if not os.path.exists(f'./outputs/models'):\n",
    "        os.makedirs(f'./outputs/models')\n",
    "    \n",
    "    best_k = 10\n",
    "    best_score = 0\n",
    "\n",
    "    for k in range(10, X_train.shape[1] + 1):\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "        X_test_selected = selector.transform(X_test)\n",
    "\n",
    "        selected_model = clone(model)\n",
    "\n",
    "        selected_model.fit(X_train_selected, y_train)\n",
    "        y_pred = selected_model.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        if accuracy > best_score:\n",
    "            best_score = accuracy\n",
    "            best_k = k\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=best_k)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "    selected = selector.get_support(indices=True)\n",
    "    selected_features = ','.join(f'\"{column_name}\"' for column_name in temp_df.columns[selected])\n",
    "\n",
    "    model_name = str(model.__class__).split(\".\")[-1].replace(\"'>\", \"\").replace(\"Classifier\", \"\")\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    if model_name == \"LGBM\":\n",
    "        study.optimize(lambda trial: objective_lgbm(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"AdaBoost\":\n",
    "        study.optimize(lambda trial: objective_ada(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        study.optimize(lambda trial: objective_dt(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"ExtraTree\":\n",
    "        study.optimize(lambda trial: objective_et(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        study.optimize(lambda trial: objective_gb(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"RandomForest\":\n",
    "        study.optimize(lambda trial: objective_rf(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"SGD\":\n",
    "        study.optimize(lambda trial: objective_sgd(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"XGB\":\n",
    "        study.optimize(lambda trial: objective_xgb(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"SVC\":\n",
    "        study.optimize(lambda trial: objective_svc(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    elif model_name == \"KNeighbors\":\n",
    "        study.optimize(lambda trial: objective_knn(trial, X_train, X_test, y_train, y_test), n_trials=10)\n",
    "        best_params = study.best_params\n",
    "    else:\n",
    "        best_params = {}\n",
    "\n",
    "    model = model.set_params(**best_params)\n",
    "\n",
    "    best_calibration_method = None\n",
    "    best_brier = 1\n",
    "    calibration_methods = ['sigmoid', 'isotonic']\n",
    "    for calibration in calibration_methods:\n",
    "        model_calibrate = clone(model)\n",
    "        calibrated_classifier = CalibratedClassifierCV(model_calibrate, method=calibration, cv=None)\n",
    "        calibrated_classifier.fit(X_train, y_train)\n",
    "        if hasattr(calibrated_classifier, 'predict_proba'):\n",
    "            prob_pos = calibrated_classifier.predict_proba(X_test)\n",
    "        else:\n",
    "            prob_pos = calibrated_classifier.decision_function(X_test)\n",
    "            prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        brier_scores = [brier_score_loss(y_test_binarized[:, i], prob_pos[:, i]) for i in range(y_test_binarized.shape[1])]\n",
    "        brier = np.mean(brier_scores)\n",
    "        if brier < best_brier:\n",
    "            best_calibration_method = calibration\n",
    "\n",
    "    model_name = f'{model_name} + {sampler_name} + k={best_k} + {best_calibration_method}'\n",
    "    calibrated_model = CalibratedClassifierCV(model, method=best_calibration_method, cv=None)\n",
    "\n",
    "    start = time.time()\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    train_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = calibrated_model.predict(X_test)\n",
    "    end = time.time()\n",
    "    test_time = end - start\n",
    "\n",
    "    train_pred = calibrated_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    #bold(f\" {model_name} \".center(100, \"#\"))\n",
    "    \n",
    "    print(f\"Model: {model_name} | Accuracy: {round(test_acc, 4) * 100}%\")\n",
    "    \n",
    "    #bold(\"#\" * 100)\n",
    "    \n",
    "    with open(f'./outputs/models/{model_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(calibrated_model, file)\n",
    "    \n",
    "    scores_df.loc[len(scores_df)] = [model_name, selected_features, best_params, train_time, test_time, train_acc, test_acc, f1, precision, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5bcc5-818a-4d48-bcb2-5da5f5b14db5",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69f496-ed3d-4da3-a0dd-c9026c060ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28f5c1-e3a5-4ed9-872f-7a4128ba150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'error',\n",
    "        'verbosity': 0,\n",
    "        'booster': 'gbtree',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-5, 10.0),\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873d928-f512-46c1-bad6-da44c03cf2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762518a-ee05-45d1-a10b-641a7013b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ada(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "    }\n",
    "\n",
    "    model = AdaBoostClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa9cb8-e5fb-410c-9de0-b4b1c90d7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gb(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be538a5-1f54-48c1-b85e-9fd5fb85575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_dt(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    model = DecisionTreeClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0011c6-b39c-4278-9b84-9a0a60f9061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_et(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    model = ExtraTreesClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729569f7-f9c4-4971-a6e2-cf9c9e9d0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_sgd(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-5, 1e-1),\n",
    "        'loss': trial.suggest_categorical('loss', ['hinge', 'log_loss', 'modified_huber', 'squared_hinge']),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
    "    }\n",
    "\n",
    "    model = SGDClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e940c-ed27-473d-923e-8e4cfed8ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svc(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf']),\n",
    "        'C': trial.suggest_loguniform('C', 1e-5, 100),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-5, 100),\n",
    "    }\n",
    "\n",
    "    model = SVC(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa92f82-909f-4c9c-9c2a-21dfbd5bb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn(trial, X_train, X_test, y_train, y_test):\n",
    "    param = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'chebyshev']),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "    }\n",
    "    model = KNeighborsClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c2d93-07ba-4ddf-abc6-cdb5015c80f6",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4fe9e-935e-4249-be22-06bac90b4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df, title):\n",
    "    plt.figure(figsize=(20, 80))\n",
    "\n",
    "    plt.subplot(711)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Train Time\", ascending=False), y=\"Model Name\", x=\"Train Time\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Train Time\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(712)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Test Time\", ascending=False), y=\"Model Name\", x=\"Test Time\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Test Time\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(713)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Train Accuracy\", ascending=True), y=\"Model Name\", x=\"Train Accuracy\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Train Accuracy\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(714)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Test Accuracy\", ascending=True), y=\"Model Name\", x=\"Test Accuracy\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Test Accuracy\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(715)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"F1\", ascending=True), y=\"Model Name\", x=\"F1\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / F1\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(716)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Precision\", ascending=True), y=\"Model Name\", x=\"Precision\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Precision\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "    plt.subplot(717)\n",
    "    ax = sns.barplot(data=df.sort_values(by=\"Recall\", ascending=True), y=\"Model Name\", x=\"Recall\", palette='viridis')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.title(\"Model / Recall\")\n",
    "    plt.xlabel(\"\")\n",
    "    save_fig(\"base_results\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb2f85-9adf-4b48-bb2d-8c88873bedf8",
   "metadata": {},
   "source": [
    "# Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc744b7-e005-4935-bc25-f41af1555fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Name', 'e_magic', 'SectionMaxEntropy', 'SectionMaxRawsize', 'SectionMaxVirtualsize', 'SectionMinPhysical', 'SectionMinVirtual', \n",
    "             'SectionMinPointerData', 'SectionMainChar', 'SuspiciousImportFunctions', 'SuspiciousNameSection', 'DirectoryEntryImportSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5f99e-477d-4c94-a34d-b423610238d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Malware'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a06dd2-75dd-43ca-8152-e9279a1f8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in drop_list:\n",
    "    if col in numerical_variables:\n",
    "        numerical_variables.remove(col)\n",
    "    elif col in categorical_variables:\n",
    "        categorical_variables.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44d1f1-e3b5-40c7-bd8e-a43e24e1e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = preprocess_and_train(train_data, drop_list, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcba43-4504-4e20-a882-8d517e716ad8",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77fc293-4bce-4480-9773-8f569f7b4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccaba7-b379-4b60-b14c-76fe808437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_df, \"Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d718f-dd47-45a8-93ff-d5e1347aaf95",
   "metadata": {},
   "source": [
    "# Model Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3c161-e25f-4c66-b604-a2f98cbc68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open(\"./outputs/models/LGBM + RandomOverSampler + k=63 + isotonic.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c823e17-7919-48d2-9b03-d240f4281fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: best_model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e562e24-3cea-4737-808d-c5ee3320ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df[\"Model Name\"] == \"LGBM + RandomOverSampler + k=63 + isotonic\"][\"Selected Features\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfbb20-5797-406c-b1d5-801dd3744108",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "\"e_cblp\",\"e_cp\",\"e_crlc\",\"e_cparhdr\",\"e_minalloc\",\"e_maxalloc\",\"e_ss\",\"e_sp\",\"e_csum\",\"e_ip\",\"e_cs\",\"e_lfarlc\",\"e_ovno\",\"e_oemid\",\"e_oeminfo\",\"e_lfanew\",\"Machine\",\"NumberOfSections\",\"TimeDateStamp\",\"PointerToSymbolTable\",\"NumberOfSymbols\",\"SizeOfOptionalHeader\",\"Characteristics\",\"Magic\",\"MajorLinkerVersion\",\"MinorLinkerVersion\",\"SizeOfCode\",\"SizeOfInitializedData\",\"SizeOfUninitializedData\",\"AddressOfEntryPoint\",\"BaseOfCode\",\"ImageBase\",\"SectionAlignment\",\"FileAlignment\",\"MajorOperatingSystemVersion\",\"MinorOperatingSystemVersion\",\"MajorImageVersion\",\"MinorImageVersion\",\"MajorSubsystemVersion\",\"MinorSubsystemVersion\",\"SizeOfHeaders\",\"CheckSum\",\"SizeOfImage\",\"Subsystem\",\"DllCharacteristics\",\"SizeOfStackReserve\",\"SizeOfStackCommit\",\"SizeOfHeapReserve\",\"SizeOfHeapCommit\",\"LoaderFlags\",\"NumberOfRvaAndSizes\",\"SectionsLength\",\"SectionMinEntropy\",\"SectionMinRawsize\",\"SectionMinVirtualsize\",\"SectionMaxPointerData\",\"SectionMaxChar\",\"DirectoryEntryImport\",\"DirectoryEntryExport\",\"ImageDirectoryEntryImport\",\"ImageDirectoryEntryResource\",\"ImageDirectoryEntryException\",\"ImageDirectoryEntrySecurity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89085e72-9fa7-4101-9d48-2ce1ad2a0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    train_data[features].astype(int).values,\n",
    "    mode='classification',\n",
    "    class_names=train_data[target].unique().tolist(),\n",
    "    training_labels=train_data[target],\n",
    "    feature_names=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc6a33-1e1a-4b9a-a318-0a471d4786b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633befc-b785-41a6-9d8d-3a62d243ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_instances(df, features, predict_fn, explainer, idx_arr):\n",
    "    for i in idx_arr:\n",
    "        bold(f\" {df.loc[i, 'Name']} \".center(100, \"#\"))\n",
    "        exp = explainer.explain_instance(df.loc[i, features].astype(int).values, predict_fn, num_features=len(features))\n",
    "        exp.show_in_notebook(show_table=True)\n",
    "        #exp.as_list()\n",
    "        bold(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f8bb7-fc31-4c9d-b7b6-bb41ff2cd6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_arr = test_data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678e9c2-ad07-4031-809b-6256e102c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_instances(test_data, features, predict_fn, explainer, idx_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddaaf9-e7a8-4a24-bf87-45f59a03fad4",
   "metadata": {},
   "source": [
    "# Try on Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fc874-2ca4-4d60-903b-bd7ddff25941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(df):\n",
    "    for i in range(len(df)):\n",
    "        file_path = str(df.loc[i, \"Name\"])\n",
    "        try:\n",
    "            pe = pefile.PE(file_path)\n",
    "        except:\n",
    "            continue\n",
    "        df.loc[i, \"e_magic\"] = pe.DOS_HEADER.e_magic\n",
    "        df.loc[i, \"e_cblp\"] = pe.DOS_HEADER.e_cblp\n",
    "        df.loc[i, \"e_cp\"] = pe.DOS_HEADER.e_cp\n",
    "        df.loc[i, \"e_crlc\"] = pe.DOS_HEADER.e_crlc\n",
    "        df.loc[i, \"e_cparhdr\"] = pe.DOS_HEADER.e_cparhdr\n",
    "        df.loc[i, \"e_minalloc\"] = pe.DOS_HEADER.e_minalloc\n",
    "        df.loc[i, \"e_maxalloc\"] = pe.DOS_HEADER.e_maxalloc\n",
    "        df.loc[i, \"e_ss\"] = pe.DOS_HEADER.e_ss\n",
    "        df.loc[i, \"e_sp\"] = pe.DOS_HEADER.e_sp\n",
    "        df.loc[i, \"e_csum\"] = pe.DOS_HEADER.e_csum\n",
    "        df.loc[i, \"e_ip\"] = pe.DOS_HEADER.e_ip\n",
    "        df.loc[i, \"e_cs\"] = pe.DOS_HEADER.e_cs\n",
    "        df.loc[i, \"e_lfarlc\"] = pe.DOS_HEADER.e_lfarlc\n",
    "        df.loc[i, \"e_ovno\"] = pe.DOS_HEADER.e_ovno\n",
    "        df.loc[i, \"e_oemid\"] = pe.DOS_HEADER.e_oemid\n",
    "        df.loc[i, \"e_oeminfo\"] = pe.DOS_HEADER.e_oeminfo\n",
    "        df.loc[i, \"e_lfanew\"] = pe.DOS_HEADER.e_lfanew\n",
    "        df.loc[i, \"Machine\"] = pe.FILE_HEADER.Machine\n",
    "        df.loc[i, \"NumberOfSections\"] = pe.FILE_HEADER.NumberOfSections\n",
    "        df.loc[i, \"TimeDateStamp\"] = pe.FILE_HEADER.TimeDateStamp\n",
    "        df.loc[i, \"PointerToSymbolTable\"] = pe.FILE_HEADER.PointerToSymbolTable\n",
    "        df.loc[i, \"NumberOfSymbols\"] = pe.FILE_HEADER.NumberOfSymbols\n",
    "        df.loc[i, \"SizeOfOptionalHeader\"] = pe.FILE_HEADER.SizeOfOptionalHeader\n",
    "        df.loc[i, \"Characteristics\"] = pe.FILE_HEADER.Characteristics\n",
    "        df.loc[i, \"Magic\"] = pe.OPTIONAL_HEADER.Magic\n",
    "        df.loc[i, \"MajorLinkerVersion\"] = pe.OPTIONAL_HEADER.MajorLinkerVersion\n",
    "        df.loc[i, \"MinorLinkerVersion\"] = pe.OPTIONAL_HEADER.MinorLinkerVersion\n",
    "        df.loc[i, \"SizeOfCode\"] = pe.OPTIONAL_HEADER.SizeOfCode\n",
    "        df.loc[i, \"SizeOfInitializedData\"] = pe.OPTIONAL_HEADER.SizeOfInitializedData\n",
    "        df.loc[i, \"SizeOfUninitializedData\"] = pe.OPTIONAL_HEADER.SizeOfUninitializedData\n",
    "        df.loc[i, \"AddressOfEntryPoint\"] = pe.OPTIONAL_HEADER.AddressOfEntryPoint\n",
    "        df.loc[i, \"BaseOfCode\"] = pe.OPTIONAL_HEADER.BaseOfCode\n",
    "        df.loc[i, \"ImageBase\"] = pe.OPTIONAL_HEADER.ImageBase\n",
    "        df.loc[i, \"SectionAlignment\"] = pe.OPTIONAL_HEADER.SectionAlignment\n",
    "        df.loc[i, \"FileAlignment\"] = pe.OPTIONAL_HEADER.FileAlignment\n",
    "        df.loc[i, \"MajorOperatingSystemVersion\"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "        df.loc[i, \"MinorOperatingSystemVersion\"] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion\n",
    "        df.loc[i, \"MajorImageVersion\"] = pe.OPTIONAL_HEADER.MajorImageVersion\n",
    "        df.loc[i, \"MinorImageVersion\"] = pe.OPTIONAL_HEADER.MinorImageVersion\n",
    "        df.loc[i, \"MajorSubsystemVersion\"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "        df.loc[i, \"MinorSubsystemVersion\"] = pe.OPTIONAL_HEADER.MinorSubsystemVersion\n",
    "        df.loc[i, \"SizeOfHeaders\"] = pe.OPTIONAL_HEADER.SizeOfHeaders\n",
    "        df.loc[i, \"CheckSum\"] = pe.OPTIONAL_HEADER.CheckSum\n",
    "        df.loc[i, \"SizeOfImage\"] = pe.OPTIONAL_HEADER.SizeOfImage\n",
    "        df.loc[i, \"Subsystem\"] = pe.OPTIONAL_HEADER.Subsystem\n",
    "        df.loc[i, \"DllCharacteristics\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "        df.loc[i, \"SizeOfStackReserve\"] = pe.OPTIONAL_HEADER.SizeOfStackReserve\n",
    "        df.loc[i, \"SizeOfStackCommit\"] = pe.OPTIONAL_HEADER.SizeOfStackCommit\n",
    "        df.loc[i, \"SizeOfHeapReserve\"] = pe.OPTIONAL_HEADER.SizeOfHeapReserve\n",
    "        df.loc[i, \"SizeOfHeapCommit\"] = pe.OPTIONAL_HEADER.SizeOfHeapCommit\n",
    "        df.loc[i, \"LoaderFlags\"] = pe.OPTIONAL_HEADER.LoaderFlags\n",
    "        df.loc[i, \"NumberOfRvaAndSizes\"] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes\n",
    "        df.loc[i, \"SectionsLength\"] = len(pe.sections)\n",
    "        \n",
    "        section_entropy_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            entropy = section.get_entropy()\n",
    "            section_entropy_dict[section_name] = entropy\n",
    "            \n",
    "        df.loc[i, \"SectionMinEntropy\"] = min(section_entropy_dict.values())\n",
    "        df.loc[i, \"SectionMaxEntropy\"] = max(section_entropy_dict.values())\n",
    "        \n",
    "        section_raw_size_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            raw_size = section.SizeOfRawData\n",
    "            section_raw_size_dict[section_name] = raw_size\n",
    "\n",
    "        df.loc[i, \"SectionMinRawsize\"] = min(section_raw_size_dict.values())\n",
    "        df.loc[i, \"SectionMaxRawsize\"] = max(section_raw_size_dict.values())\n",
    "        \n",
    "        section_virt_size_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            virt_size = section.Misc_VirtualSize\n",
    "            section_virt_size_dict[section_name] = virt_size\n",
    "            \n",
    "        df.loc[i, \"SectionMinVirtualsize\"] = min(section_virt_size_dict.values())\n",
    "        df.loc[i, \"SectionMaxVirtualsize\"] = max(section_virt_size_dict.values())\n",
    "        \n",
    "        section_physical_addr_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            physical = section.Misc_PhysicalAddress\n",
    "            section_physical_addr_dict[section_name] = physical\n",
    "            \n",
    "        df.loc[i, \"SectionMaxPhysical\"] = max(section_physical_addr_dict.values())\n",
    "        df.loc[i, \"SectionMinPhysical\"] = min(section_physical_addr_dict.values())\n",
    "        \n",
    "        section_virt_addr_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            virtual = section.VirtualAddress\n",
    "            section_virt_addr_dict[section_name] = virtual\n",
    "    \n",
    "        df.loc[i, \"SectionMaxVirtual\"] = max(section_virt_addr_dict.values())\n",
    "        df.loc[i, \"SectionMinVirtual\"] = min(section_virt_addr_dict.values())\n",
    "        \n",
    "        section_pointer_data_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            pointer_data = section.PointerToRawData\n",
    "            section_pointer_data_dict[section_name] = pointer_data\n",
    "            \n",
    "        df.loc[i, \"SectionMaxPointerData\"] = max(section_pointer_data_dict.values())\n",
    "        df.loc[i, \"SectionMinPointerData\"] = min(section_pointer_data_dict.values())\n",
    "\n",
    "        section_char_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            chars = section.Characteristics\n",
    "            section_char_dict[section_name] = chars\n",
    "            \n",
    "        df.loc[i, \"SectionMaxChar\"] = max(section_char_dict.values())\n",
    "        df.loc[i, \"SectionMainChar\"] = min(section_char_dict.values())\n",
    "        \n",
    "        try:\n",
    "            df.loc[i, \"DirectoryEntryImport\"] = len(pe.DIRECTORY_ENTRY_IMPORT)\n",
    "        except:\n",
    "            df.loc[i, \"DirectoryEntryImport\"] = 0\n",
    "        try:\n",
    "            df.loc[i, \"DirectoryEntryExport\"] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)\n",
    "        except:\n",
    "            df.loc[i, \"DirectoryEntryExport\"] = 0\n",
    "        \n",
    "        df.loc[i, \"ImageDirectoryEntryExport\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryImport\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryResource\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryException\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXCEPTION']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntrySecurity\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].Size\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee8cf2-1771-4c4a-9127-0b8e3835d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget \"https://www.python.org/ftp/python/3.12.0/python-3.12.0-amd64.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc034b-e758-42b5-b5f3-f252a24a93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = pickle.load(open(\"./outputs/process/rs.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88292f80-9b04-4df2-9ded-dc54b5e0cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'Name', 'e_magic', 'SectionMaxEntropy', 'SectionMaxRawsize', \n",
    "    'SectionMaxVirtualsize', 'SectionMinPhysical', 'SectionMinVirtual', \n",
    "    'SectionMinPointerData', 'SectionMainChar'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d7564-ab5c-4130-900d-845cb6c3c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_file(file_path, features, drop_cols):\n",
    "    test_df = pd.DataFrame({\"Name\": [file_path]})\n",
    "    result_df = analyze(test_df)\n",
    "    test_df = result_df.drop(drop_cols, axis=1)\n",
    "    test = rs.transform(test_df)\n",
    "    test = pd.DataFrame(test, columns=test_df.columns)\n",
    "    test = test[features]\n",
    "    result = best_model.predict_proba(test)\n",
    "    if np.argmax(result) == 1:\n",
    "        print(\"[-] This file is malicious.\")\n",
    "    else:\n",
    "        print(\"[+] This file is benign.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555b7d5-41be-4b17-b811-39b376ba7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./test/abc.exe\"\n",
    "test_file(file, features, drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af3134-8de7-43ba-91f7-e4eeba476346",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c7cf9-40d6-4ff6-98e1-514c3436f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.drop(columns=['Selected Features', 'Parameters']).sort_values(by='Test Accuracy', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23447210-ff1c-4415-b189-80980e28db21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
