{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pefile\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore, Style, Back\n",
    "\n",
    "colorama.init(autoreset=True)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from scikitplot.metrics import plot_roc_curve\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def bold(string):\n",
    "    display(Markdown(\"**\" + string + \"**\"))\n",
    "\n",
    "PROJECT_ROOT_DIR = \"..\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "np.random.seed(42)\n",
    "\n",
    "def save_fig(title):\n",
    "    path = os.path.join(IMAGES_PATH, title + \".png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, format=\"png\", dpi=300)\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../dataset/file_pe.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop([\"Name\", \"SuspiciousImportFunctions\", \"SuspiciousNameSection\", \"DirectoryEntryImportSize\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(data):\n",
    "    bold(\"**\" + \" SHAPE \".center(50, \"#\") + \"**\")\n",
    "    print(\"ROWS: {}\".format(data.shape[0]))\n",
    "    print(\"COLUMNS: {}\".format(data.shape[1]))\n",
    "    bold(\"**\" + \" TYPES \".center(50, \"#\") + \"**\")\n",
    "    print(data.dtypes)\n",
    "    bold(\"**\" + \" MISSING VALUES \".center(50, \"#\") + \"**\")\n",
    "    print(data.isnull().sum())\n",
    "    bold(\"**\" + \" DUPLICATED VALUES \".center(50, \"#\") + \"**\")\n",
    "    print(\"NUMBER OF DUPLICATED VALUES: {}\".format(data.duplicated().sum()))\n",
    "    bold(\"**\" + \" MEMORY USAGE \".center(50, \"#\") + \"**\")\n",
    "    buf = io.StringIO()\n",
    "    data.info(buf=buf)\n",
    "    info = buf.getvalue().split(\"\\n\")[-2].split(\":\")[1].strip()\n",
    "    print(\"Memory Usage: {}\".format(info))\n",
    "    #bold(\"**\" + \" DESCRIBE \".center(50, \"#\") + \"**\")\n",
    "    #print(data.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Malware\"\n",
    "numerical_variables = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col != target]\n",
    "categorical_variables = [col for col in df.columns if pd.api.types.is_categorical_dtype(df[col]) or df[col].dtype == \"O\" and col != target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold(\"NUMERICAL VARIABLES\")\n",
    "print(numerical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold(\"CATEGORICAL VARIABLES\")\n",
    "print(categorical_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TARGET VALUE DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(df, col, title):\n",
    "    fig, ax = plt.subplots(1 ,2, figsize=(18, 6))\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    values = df[col].value_counts()\n",
    "    N = len(values)\n",
    "\n",
    "    outer_pie = values\n",
    "    inner_pie = values / N\n",
    "\n",
    "    ax[0].pie(\n",
    "        outer_pie,\n",
    "        labels=values.index.tolist(),\n",
    "        startangle=90,\n",
    "        frame=False,\n",
    "        radius=1.3,\n",
    "        explode=([0.05] * (N-1) + [0.3]),\n",
    "        wedgeprops={\"linewidth\": 1, \"edgecolor\": \"white\"},\n",
    "        textprops={\"fontsize\": 12, \"weight\": \"bold\"}\n",
    "    )\n",
    "\n",
    "    ax[0].pie(\n",
    "        inner_pie,\n",
    "        radius=1,\n",
    "        startangle=90,\n",
    "        autopct=\"%1.f%%\",\n",
    "        explode=([0.1] * (N-1) + [0.3]),\n",
    "        pctdistance=0.8,\n",
    "        textprops={\"size\": 13, \"weight\": \"bold\", \"color\": \"white\"}\n",
    "    )\n",
    "\n",
    "    center_circle = plt.Circle((0, 0), 0.7, color=\"black\", fc=\"white\", linewidth=0)\n",
    "    ax[0].add_artist(center_circle)\n",
    "\n",
    "    sns.barplot(x=values, y=values.index.tolist(), orient=\"horizontal\")\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        ax[1].text(v, i, str(v), color=\"black\", fontweight=\"bold\", fontsize=13)\n",
    "\n",
    "    plt.setp(ax[1].get_yticklabels(), fontweight=\"bold\")\n",
    "    plt.setp(ax[1].get_xticklabels(), fontweight=\"bold\")\n",
    "    ax[1].set_xlabel(col, fontweight=\"bold\", color=\"black\")\n",
    "    ax[1].set_ylabel(\"count\", fontweight=\"bold\", color=\"black\")\n",
    "\n",
    "    fig.suptitle(f\"{title}\", fontsize=18, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    save_fig(\"target_value_distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count(df, \"Malware\", f\"Malware Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num(df, columns):\n",
    "    for i, column in enumerate(columns):\n",
    "        plt.subplot(int(len(columns) / 2) + 1, 2, i + 1)\n",
    "        sns.histplot(x=column, data=df, bins=30, kde=True)\n",
    "        plt.axvline(df[column].mean(), color=\"r\", linestyle=\"--\", label=\"Mean\")\n",
    "        plt.axvline(df[column].median(), color=\"g\", linestyle=\"-\", label=\"Median\")\n",
    "        plt.grid()\n",
    "        plt.title(f\"{column} Distribution\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "    save_fig(\"numerical_variable_distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, len(numerical_variables) * 2.5))\n",
    "plot_num(df, numerical_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "X = df.drop(\"Malware\", axis=1)\n",
    "y = df[\"Malware\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models[\"Random Forest\"] = RandomForestClassifier()\n",
    "models[\"AdaBoost\"] = AdaBoostClassifier()\n",
    "models[\"Gradient Boosting\"] = GradientBoostingClassifier()\n",
    "models[\"LinearSVC\"] = LinearSVC()\n",
    "models[\"SVC\"] = SVC()\n",
    "models[\"Decision Tree\"] = DecisionTreeClassifier()\n",
    "models[\"Extra Tree\"] = ExtraTreeClassifier()\n",
    "models[\"Logistic Regression\"] = LogisticRegression()\n",
    "models[\"SGD\"] = SGDClassifier()\n",
    "models[\"XGB\"] = XGBClassifier()\n",
    "models[\"LGBM\"] = LGBMClassifier(verbose=0)\n",
    "models[\"CatBoost\"] = CatBoostClassifier(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time, test_time, accuracy, precision, recall, f1 = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "for key in models.keys():\n",
    "    start_time = time.time()\n",
    "    models[key].fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    train_time[key] = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    predictions = models[key].predict(X_test)\n",
    "    end_time = time.time()\n",
    "    test_time[key] = end_time - start_time\n",
    "\n",
    "    accuracy[key] = accuracy_score(y_test, predictions)\n",
    "    precision[key] = precision_score(y_test, predictions)\n",
    "    recall[key] = recall_score(y_test, predictions)\n",
    "    f1[key] = f1_score(y_test, predictions)\n",
    "\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    pkl_filename = f\"../models/{key}.pkl\"\n",
    "    with open(pkl_filename, \"wb\") as file:\n",
    "        pickle.dump(models[key], file)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plot_confusion_matrix(conf_mat=cm, show_absolute=True, show_normed=True, colorbar=True, figsize=(8, 8))\n",
    "    plt.title(f\"{key}\")\n",
    "    save_fig(f\"{key} Confusion Matrix\")\n",
    "\n",
    "    classification_report(y_test, predictions, target_names=[\"benign\", \"malware\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"Train Time\": train_time.values(),\n",
    "                           \"Test Time\": test_time.values(),\n",
    "                           \"Accuracy\": accuracy.values(),\n",
    "                           \"Precision\": precision.values(),\n",
    "                           \"Recall\": recall.values(),\n",
    "                           \"F1\": f1.values()}, index=models.keys())\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df.plot.barh()\n",
    "ax.legend(\n",
    "    ncol=len(models.keys()), \n",
    "    bbox_to_anchor=(0, 1), \n",
    "    loc='lower left', \n",
    "    prop={'size': 7}\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 25))\n",
    "\n",
    "plt.subplot(611)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"Accuracy\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / Accuracy Score\")\n",
    "\n",
    "plt.subplot(612)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"F1\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / F1 Score\")\n",
    "\n",
    "plt.subplot(613)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"Precision\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / Precision Score\")\n",
    "\n",
    "plt.subplot(614)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"Recall\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / Recall Score\")\n",
    "\n",
    "plt.subplot(615)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"Train Time\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / Train Time\")\n",
    "\n",
    "plt.subplot(616)\n",
    "ax = sns.barplot(data=results_df, x=results_df.index, y=\"Test Time\")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "plt.title(\"Model / Test Time\")\n",
    "\n",
    "save_fig(\"results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(df):\n",
    "    for i in range(len(df)):\n",
    "        file_path = str(df.loc[i, \"Name\"])\n",
    "        try:\n",
    "            pe = pefile.PE(file_path)\n",
    "        except:\n",
    "            continue\n",
    "        df.loc[i, \"e_magic\"] = pe.DOS_HEADER.e_magic\n",
    "        df.loc[i, \"e_cblp\"] = pe.DOS_HEADER.e_cblp\n",
    "        df.loc[i, \"e_cp\"] = pe.DOS_HEADER.e_cp\n",
    "        df.loc[i, \"e_crlc\"] = pe.DOS_HEADER.e_crlc\n",
    "        df.loc[i, \"e_cparhdr\"] = pe.DOS_HEADER.e_cparhdr\n",
    "        df.loc[i, \"e_minalloc\"] = pe.DOS_HEADER.e_minalloc\n",
    "        df.loc[i, \"e_maxalloc\"] = pe.DOS_HEADER.e_maxalloc\n",
    "        df.loc[i, \"e_ss\"] = pe.DOS_HEADER.e_ss\n",
    "        df.loc[i, \"e_sp\"] = pe.DOS_HEADER.e_sp\n",
    "        df.loc[i, \"e_csum\"] = pe.DOS_HEADER.e_csum\n",
    "        df.loc[i, \"e_ip\"] = pe.DOS_HEADER.e_ip\n",
    "        df.loc[i, \"e_cs\"] = pe.DOS_HEADER.e_cs\n",
    "        df.loc[i, \"e_lfarlc\"] = pe.DOS_HEADER.e_lfarlc\n",
    "        df.loc[i, \"e_ovno\"] = pe.DOS_HEADER.e_ovno\n",
    "        df.loc[i, \"e_oemid\"] = pe.DOS_HEADER.e_oemid\n",
    "        df.loc[i, \"e_oeminfo\"] = pe.DOS_HEADER.e_oeminfo\n",
    "        df.loc[i, \"e_lfanew\"] = pe.DOS_HEADER.e_lfanew\n",
    "        df.loc[i, \"Machine\"] = pe.FILE_HEADER.Machine\n",
    "        df.loc[i, \"NumberOfSections\"] = pe.FILE_HEADER.NumberOfSections\n",
    "        df.loc[i, \"TimeDateStamp\"] = pe.FILE_HEADER.TimeDateStamp\n",
    "        df.loc[i, \"PointerToSymbolTable\"] = pe.FILE_HEADER.PointerToSymbolTable\n",
    "        df.loc[i, \"NumberOfSymbols\"] = pe.FILE_HEADER.NumberOfSymbols\n",
    "        df.loc[i, \"SizeOfOptionalHeader\"] = pe.FILE_HEADER.SizeOfOptionalHeader\n",
    "        df.loc[i, \"Characteristics\"] = pe.FILE_HEADER.Characteristics\n",
    "        df.loc[i, \"Magic\"] = pe.OPTIONAL_HEADER.Magic\n",
    "        df.loc[i, \"MajorLinkerVersion\"] = pe.OPTIONAL_HEADER.MajorLinkerVersion\n",
    "        df.loc[i, \"MinorLinkerVersion\"] = pe.OPTIONAL_HEADER.MinorLinkerVersion\n",
    "        df.loc[i, \"SizeOfCode\"] = pe.OPTIONAL_HEADER.SizeOfCode\n",
    "        df.loc[i, \"SizeOfInitializedData\"] = pe.OPTIONAL_HEADER.SizeOfInitializedData\n",
    "        df.loc[i, \"SizeOfUninitializedData\"] = pe.OPTIONAL_HEADER.SizeOfUninitializedData\n",
    "        df.loc[i, \"AddressOfEntryPoint\"] = pe.OPTIONAL_HEADER.AddressOfEntryPoint\n",
    "        df.loc[i, \"BaseOfCode\"] = pe.OPTIONAL_HEADER.BaseOfCode\n",
    "        df.loc[i, \"ImageBase\"] = pe.OPTIONAL_HEADER.ImageBase\n",
    "        df.loc[i, \"SectionAlignment\"] = pe.OPTIONAL_HEADER.SectionAlignment\n",
    "        df.loc[i, \"FileAlignment\"] = pe.OPTIONAL_HEADER.FileAlignment\n",
    "        df.loc[i, \"MajorOperatingSystemVersion\"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "        df.loc[i, \"MinorOperatingSystemVersion\"] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion\n",
    "        df.loc[i, \"MajorImageVersion\"] = pe.OPTIONAL_HEADER.MajorImageVersion\n",
    "        df.loc[i, \"MinorImageVersion\"] = pe.OPTIONAL_HEADER.MinorImageVersion\n",
    "        df.loc[i, \"MajorSubsystemVersion\"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "        df.loc[i, \"MinorSubsystemVersion\"] = pe.OPTIONAL_HEADER.MinorSubsystemVersion\n",
    "        df.loc[i, \"SizeOfHeaders\"] = pe.OPTIONAL_HEADER.SizeOfHeaders\n",
    "        df.loc[i, \"CheckSum\"] = pe.OPTIONAL_HEADER.CheckSum\n",
    "        df.loc[i, \"SizeOfImage\"] = pe.OPTIONAL_HEADER.SizeOfImage\n",
    "        df.loc[i, \"Subsystem\"] = pe.OPTIONAL_HEADER.Subsystem\n",
    "        df.loc[i, \"DllCharacteristics\"] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "        df.loc[i, \"SizeOfStackReserve\"] = pe.OPTIONAL_HEADER.SizeOfStackReserve\n",
    "        df.loc[i, \"SizeOfStackCommit\"] = pe.OPTIONAL_HEADER.SizeOfStackCommit\n",
    "        df.loc[i, \"SizeOfHeapReserve\"] = pe.OPTIONAL_HEADER.SizeOfHeapReserve\n",
    "        df.loc[i, \"SizeOfHeapCommit\"] = pe.OPTIONAL_HEADER.SizeOfHeapCommit\n",
    "        df.loc[i, \"LoaderFlags\"] = pe.OPTIONAL_HEADER.LoaderFlags\n",
    "        df.loc[i, \"NumberOfRvaAndSizes\"] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes\n",
    "        df.loc[i, \"SectionsLength\"] = len(pe.sections)\n",
    "        \n",
    "        section_entropy_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            entropy = section.get_entropy()\n",
    "            section_entropy_dict[section_name] = entropy\n",
    "            \n",
    "        df.loc[i, \"SectionMinEntropy\"] = min(section_entropy_dict.values())\n",
    "        df.loc[i, \"SectionMaxEntropy\"] = max(section_entropy_dict.values())\n",
    "        \n",
    "        section_raw_size_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            raw_size = section.SizeOfRawData\n",
    "            section_raw_size_dict[section_name] = raw_size\n",
    "\n",
    "        df.loc[i, \"SectionMinRawsize\"] = min(section_raw_size_dict.values())\n",
    "        df.loc[i, \"SectionMaxRawsize\"] = max(section_raw_size_dict.values())\n",
    "        \n",
    "        section_virt_size_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            virt_size = section.Misc_VirtualSize\n",
    "            section_virt_size_dict[section_name] = virt_size\n",
    "            \n",
    "        df.loc[i, \"SectionMinVirtualsize\"] = min(section_virt_size_dict.values())\n",
    "        df.loc[i, \"SectionMaxVirtualsize\"] = max(section_virt_size_dict.values())\n",
    "        \n",
    "        section_physical_addr_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            physical = section.Misc_PhysicalAddress\n",
    "            section_physical_addr_dict[section_name] = physical\n",
    "            \n",
    "        df.loc[i, \"SectionMaxPhysical\"] = max(section_physical_addr_dict.values())\n",
    "        df.loc[i, \"SectionMinPhysical\"] = min(section_physical_addr_dict.values())\n",
    "        \n",
    "        section_virt_addr_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            virtual = section.VirtualAddress\n",
    "            section_virt_addr_dict[section_name] = virtual\n",
    "    \n",
    "        df.loc[i, \"SectionMaxVirtual\"] = max(section_virt_addr_dict.values())\n",
    "        df.loc[i, \"SectionMinVirtual\"] = min(section_virt_addr_dict.values())\n",
    "        \n",
    "        section_pointer_data_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            pointer_data = section.PointerToRawData\n",
    "            section_pointer_data_dict[section_name] = pointer_data\n",
    "            \n",
    "        df.loc[i, \"SectionMaxPointerData\"] = max(section_pointer_data_dict.values())\n",
    "        df.loc[i, \"SectionMinPointerData\"] = min(section_pointer_data_dict.values())\n",
    "\n",
    "        section_char_dict = {}\n",
    "        for section in pe.sections:\n",
    "            section_name = section.Name.decode('utf-8').strip('\\x00')\n",
    "            chars = section.Characteristics\n",
    "            section_char_dict[section_name] = chars\n",
    "            \n",
    "        df.loc[i, \"SectionMaxChar\"] = max(section_char_dict.values())\n",
    "        df.loc[i, \"SectionMainChar\"] = min(section_char_dict.values())\n",
    "        \n",
    "        try:\n",
    "            df.loc[i, \"DirectoryEntryImport\"] = len(pe.DIRECTORY_ENTRY_IMPORT)\n",
    "        except:\n",
    "            df.loc[i, \"DirectoryEntryImport\"] = 0\n",
    "        try:\n",
    "            df.loc[i, \"DirectoryEntryExport\"] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)\n",
    "        except:\n",
    "            df.loc[i, \"DirectoryEntryExport\"] = 0\n",
    "        \n",
    "        df.loc[i, \"ImageDirectoryEntryExport\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryImport\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryResource\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntryException\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXCEPTION']].Size\n",
    "        df.loc[i, \"ImageDirectoryEntrySecurity\"] = pe.OPTIONAL_HEADER.DATA_DIRECTORY[pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']].Size\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(file_path, models, scan_file):\n",
    "    test_df = pd.DataFrame({\"Name\": file_path})\n",
    "    result_df = analyze(test_df)\n",
    "    test = result_df.drop(\"Name\", axis=1)\n",
    "    if scan_file:\n",
    "        print(f\"[***] File: {Style.BRIGHT}{file_path}{Style.NORMAL} [***]\")\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        total_benign, total_malicious = 0, 0\n",
    "        for key in models.keys():\n",
    "            result = models[key].predict(test)\n",
    "            if result[i] == 0:\n",
    "                total_benign += 1\n",
    "                if scan_file:\n",
    "                    print(f\"{Fore.GREEN}[+]{Fore.RESET} Model {Style.BRIGHT}{key}{Style.NORMAL} labeled {Fore.GREEN}benign{Fore.RESET}.\")\n",
    "            else:\n",
    "                total_malicious += 1\n",
    "                if scan_file:\n",
    "                    print(f\"{Fore.RED}[-]{Fore.RESET} Model {Style.BRIGHT}{key}{Style.NORMAL} labeled {Fore.RED}malware{Fore.RESET}.\")\n",
    "    \n",
    "        if total_benign > total_malicious:\n",
    "            if not scan_file:\n",
    "                print(f\"[*] Scanning file: {file_path[i]}.\")\n",
    "            print(f\"{Fore.YELLOW}[=] File {Fore.RESET}{Back.GREEN}{round((total_benign / 12), 2)}%{Back.RESET}{Fore.YELLOW} is benign{Fore.RESET}.\")\n",
    "\n",
    "        elif total_malicious > total_benign:\n",
    "            if not scan_file:\n",
    "                print(f\"[*] Scanning file: {file_path[i]}.\")\n",
    "            print(f\"{Fore.YELLOW}[=] File {Fore.RESET}{Back.RED}{round((total_malicious / 12), 2)}%{Back.RESET}{Fore.YELLOW} is malicious{Fore.RESET}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test([\"../test/python-3.12.0-amd64.exe\", \"../test/python-3.12.0-amd64.exe\"], models, scan_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
